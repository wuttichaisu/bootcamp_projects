{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Price scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import string \n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Scraping Data\n",
    "def ScrapingData(url):\n",
    "    k = requests.get(url).text\n",
    "    soup=BeautifulSoup(k,'html.parser')  \n",
    "    productlist = soup.find_all(\"div\",{\"class\":\"pagination_pagination__RUsgT\"})\n",
    "    if not productlist:\n",
    "        page = 1\n",
    "    else:\n",
    "        page = productlist[0].find_all(\"a\")\n",
    "        page = page[-1].find(\"span\").text\n",
    "\n",
    "    name = []\n",
    "    link = []\n",
    "    base_price = []\n",
    "    price = []\n",
    "    prefix_link = url.replace('https://','')\n",
    "    prefix_link = prefix_link.replace('http://','')\n",
    "    prefix_link = prefix_link.split('/')[0]\n",
    "\n",
    "    for i in range(1,int(page)+1):\n",
    "        CurrentURL = url + \"?page=\" + str(i)\n",
    "        k = requests.get(CurrentURL).text\n",
    "        soup=BeautifulSoup(k,'html.parser')  \n",
    "        productlist = soup.find_all(\"div\",{\"class\":\"category_result_row__qfMRp\"})\n",
    "\n",
    "        base_prices = productlist[0].find_all(\"span\",{\"class\":\"productCard_base_price__j5_Sq\"})\n",
    "        prices = productlist[0].find_all(\"div\",{\"class\":\"productCard_price__zid5Z\"})\n",
    "        products = productlist[0].find_all(\"div\",{\"class\":\"productCard_title__HUSZQ\"})\n",
    "\n",
    "        for i,j,k in zip(products,base_prices,prices):\n",
    "            name.append(i.find('a').text)\n",
    "            link.append(prefix_link + i.find(\"a\").get('href'))\n",
    "            p = k.text.split('/')\n",
    "            price.append(p[0])\n",
    "            if (not j.text):\n",
    "                base_price.append(p[0])\n",
    "            else:\n",
    "                base_price.append(j.text)\n",
    "    output = {}\n",
    "    output['name'] = name\n",
    "    output['base_price'] = base_price\n",
    "    output['price'] = price\n",
    "    output['link'] = link\n",
    "    output['webname'] = prefix_link.split('.')[1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function scrap specific data\n",
    "def ScrapingSpecificData(link):\n",
    "    if (link.find(\"http://\") == -1) & (link.find(\"https://\") == -1):\n",
    "        link = 'http://'+link\n",
    "    k = requests.get(link).text\n",
    "    soup=BeautifulSoup(k,'html.parser')\n",
    "    name = soup.find_all('h1',{'class':'productDetail_product_title__fb0ow'})[0].text\n",
    "    pack_detail = soup.find_all('span',{\"class\":\"productDetail_unit__QntiB\"})\n",
    "    detail = soup.find_all(\"div\",{\"class\":\"productDetail_category__mbviF\"})\n",
    "    for i in string.punctuation:\n",
    "        name = name.replace(i,'')\n",
    "    name = name.split(' ')\n",
    "    output = {}\n",
    "\n",
    "    if not pack_detail:\n",
    "        pack = ''\n",
    "    else:\n",
    "        pack = pack_detail[0].text[2:]\n",
    "    for n in name:\n",
    "        if (n.isdigit()) & (not name[-1].isdigit()):\n",
    "            pack = name[-1]\n",
    "\n",
    "    if ('กก' in pack) | ('กิโลกรัม' in pack):\n",
    "        pack = 'KG'\n",
    "    elif ('ก' in pack) | ('กรัม' in pack):\n",
    "        pack = 'G'\n",
    "    output['Pack'] = pack\n",
    "\n",
    "    if (len(name) > 2):\n",
    "        if name[-2].isdigit():\n",
    "            output['Unit'] = name[-2]\n",
    "        else:\n",
    "            output['Unit'] = 1\n",
    "    else:\n",
    "        output['Unit'] = 1\n",
    "\n",
    "    if not detail:\n",
    "        brand = ''\n",
    "        category = ''\n",
    "    else:\n",
    "        brand = (detail[0].find(\"b\"))\n",
    "        category = (detail[0].find(\"a\"))\n",
    "    if not brand:\n",
    "        output['Brand'] = ''\n",
    "    else:\n",
    "        output['Brand'] = brand.text\n",
    "    if not category:\n",
    "        output['Product'] = ''\n",
    "    else:\n",
    "        output['Product'] = category.text\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data \n",
    "dic = {'Date':[],\n",
    "       'Product':[],\n",
    "       'Product Group':[],\n",
    "       'Store':[],\n",
    "       'Product Name':[],\n",
    "       'Brand':[],\n",
    "       'Pack':[],\n",
    "       'Unit':[],\n",
    "       'Original Price':[],\n",
    "       'Final Price':[],\n",
    "       'url':[]}\n",
    "\n",
    "group = ['หมู','ไก่','marinated','ไส้กรอก','แฮม','บาโลน่า','เบคอน','ลูกชิ้น','เนื้อสัตว์หมักปรุงรส']\n",
    "\n",
    "URLs = pd.read_csv('BigC_Input_URL.csv') #open file URL here\n",
    "   \n",
    "for url in URLs.url:\n",
    "    x = ScrapingData(url)\n",
    "    dic['Product Name'].extend(x['name'])\n",
    "    dic['Original Price'].extend(x['base_price'])\n",
    "    dic['Final Price'].extend(x['price'])\n",
    "    dic['url'].extend(x['link'])\n",
    "\n",
    "for link in dic['url']:\n",
    "    y = ScrapingSpecificData(link)\n",
    "    dic['Date'].append(datetime.now().strftime('%d-%m-%y %H:%M'))\n",
    "    dic['Store'].append('Big-C')\n",
    "    dic['Product'].append(y['Product'])\n",
    "    dic['Brand'].append(y['Brand'])\n",
    "    dic['Pack'].append(y['Pack'])\n",
    "    dic['Unit'].append(str(y['Unit']))\n",
    "    selct_group = ''\n",
    "    for g in group:\n",
    "        if g in y['Product']:\n",
    "            selct_group = g\n",
    "            break\n",
    "    dic['Product Group'].append(selct_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to excel\n",
    "df = pd.DataFrame(dic)\n",
    "df.to_csv(x['webname']+'_Ouput_'+str(date.today())+'.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb009dd1c8451fb55ba972b607cbcd9dcffc25994d0a7a2f1c390b5620c5e0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
